# DeepSeekR1本地RAG知识库(13): Embedding模型讲解(小白也能懂)

你是不是不知道怎么选择Embedding向量模型？

也不知道该如何准备本地知识库的资料？

更不知道Embedding向量模型到底干啥用的？或者它是怎么工作的呢？

今天我就带领大家一起来看看他的奥秘。

## 一、背景

当我们使用检索增强生成（RAG）技术来搭建本地知识库时，Embedding 模型就如同这个图书馆的智能索引系统，能让我们快速准确地找到所需的知识。

对于刚接触这一领域的小白来说，Embedding 模型到底是什么，它在本地知识库中扮演着怎样的角色，可能还比较模糊。近期的本地知识库的课程，很多粉丝也反映了上面的问题。

接下来，我们就用通俗易懂的语言，结合实际例子，详细探讨这些问题，同时分享如何准备本地知识，让本地 RAG 回答问题更加准确和全面。

## 二、Embedding 模型是什么

简单来讲，Embedding 就像是给数据穿上了一件“数字外衣”，把原本各种各样的数据（比如文本、图像、语音等）转化成一组数字，也就是向量来表示。

![](https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/1c78473490e04fc7a2aff591d5440f41~tplv-tt-origin-web:gif.jpeg?_iz=58558&from=article.pc_detail&lk3s=953192f4&x-expires=1743230948&x-signature=RVgpU%2Bqg3MIIrQH3tlrT%2BBXFJk4%3D)

Embedding向量本质上是将语义信息映射到高维空间的数学坐标。以3D空间为例：

- "猫" → [0.7, -0.3, 0.1]
- "犬" → [0.68, -0.25, 0.15]
- "发动机" → [-0.4, 0.8, 0.5]

### 2.1 维度解释

- **低维（<100）**：语义区分能力弱，可能出现"苹果"（水果）与"苹果"（手机）混淆
- **高维（>1000）**：需要更多计算资源，但能捕捉"跑步"与"慢跑"的细微差别

**实际模型维度示例**

```python
print("BGE维度:", len(model.encode("文本示例")[0])) # 输出: 1024
print("Jina维度:", len(jina_model.encode("示例")[0])) # 输出: 768
```

### 2.2 常用模型深度对比


| 特性         | BGE-large-zh-v1.5 | Jina-embeddings-v2 | Nomic-embed-text-v1.5 |
| ------------ | ----------------- | ------------------ | --------------------- |
| 向量维度     | 1024              | 768                | 768                   |
| 最大文本长度 | 512 tokens        | 8192 tokens        | 2048 tokens           |
| 推荐距离方法 | 余弦相似度        | 点积（需归一化）   | 余弦相似度            |
| 训练数据量   | 50亿中文token     | 1.2万亿多语言token | 2亿英文文档           |
| 多语言支持   | 中文优先          | 支持100+语言       | 英语专用              |
| 模型体积     | 1.2GB             | 890MB              | 420MB                 |
| 推理内存需求 | 3GB               | 2.5GB              | 1.8GB                 |
| 温度参数     | 不支持            | 支持动态温度调节   | 固定温度0.8           |
| 领域适配能力 | 法律/金融专用版   | 通用领域           | 科技论文优化          |

在本地知识库搭建中，我们重点关注的是文本 Embedding，它能把文本信息变成计算机更容易理解和处理的向量形式。

## 三、Embedding 模型的原理

### 3.1. 词向量化

**独热编码（one-hot encoding）**
独热编码是一种简单直接的词向量化方法。想象我们有一个水果词汇表，里面只有“苹果”“香蕉”“橘子”三个词。我们可以给每个词分配一个二进制向量，这个向量只有一个位置是 1，其余位置都是 0。例如：

- “苹果”表示为 [1, 0, 0]
- “香蕉”表示为 [0, 1, 0]
- “橘子”表示为 [0, 0, 1]

这种表示有明显的缺点，如果词汇表变得非常大，这样的向量会非常长，而且无法体现词与词之间的语义关系，比如“苹果”和“香蕉”都属于水果，在这种表示方法下它们看起来毫无关联。

**词嵌入（word embeddings）**
词嵌入是一种更高级的词向量化方法，像 word2vec、glove 等模型都属于词嵌入模型。

以 word2vec 为例，它通过预测词的上下文来学习词向量。假设我们有这样一个句子“我喜欢吃苹果”，word2vec 会根据“苹果”周围的词（“喜欢”“吃”）来学习“苹果”的向量表示。

最终，语义相似的词在向量空间中会靠得很近。比如“苹果”和“香蕉”，它们的向量在空间中的位置会比较接近，因为它们都是水果。

### 3.2. 句子向量化

**简单平均/加权平均**
简单平均就是把句子中每个词的向量加起来，然后除以词的数量，得到句子的向量。例如，句子“我爱祖国”，假设“我”的向量是 [0.1, 0.2, 0.3]，“爱”的向量是 [0.4, 0.5, 0.6]，“祖国”的向量是 [0.7, 0.8, 0.9]，那么句子向量就是这三个向量相加后除以 3。加权平均则是根据词的重要性给每个词的向量赋予不同的权重，再进行计算。比如在一个句子中，关键词的权重可以设得高一些。

**递归神经网络（RNN）**
RNN 就像一个有记忆力的小机器人，它会按照句子中词的顺序依次处理每个词，并且会记住之前处理过的信息。例如，对于句子“我今天去超市买了苹果”，RNN 会先处理“我”，然后结合“我”的信息处理“今天”，以此类推，最终生成整个句子的向量表示。能够更好地处理长句子，因为它们有特殊的机制来避免在处理长序列时丢失重要信息。

**卷积神经网络（CNN）**
CNN 就像一个拿着放大镜的检查员，它会在句子中滑动一个小窗口（卷积核），捕捉句子中的局部特征。比如在句子“这家餐厅的披萨非常好吃”中，CNN 可能会通过卷积核发现“披萨好吃”这样的局部特征，然后根据这些特征生成句子向量。

**自注意力机制（如 Transformer）**
自注意力机制就像是一个聪明的秘书，它会关注句子中每个词与其他词的关系。对于句子“小明和小红一起去公园玩”，会分析“小明”和“小红”之间的关系，以及它们和“公园”“玩”的关系，然后综合这些信息生成句子向量。这样就能更好地捕捉句子中词与词之间复杂的语义关系。

### 3.3. 文档向量化

**简单平均/加权平均**
和句子向量化类似，对文档中的每个句子向量进行平均或加权平均，得到文档的向量表示。比如一篇关于旅游的文档，有多个句子描述不同的景点，将这些句子向量平均后就得到了文档向量。

**文档主题模型**
就像一个分类大师，它会分析文档中词的分布，找出文档的主题。例如，一篇文档中多次出现“足球”“比赛”“球员”等词，可能会判断这篇文档的主题是足球。然后根据主题分布生成文档的向量表示。

**层次化模型（如 doc2vec）**
doc2vec 是 word2vec 的扩展，它不仅考虑了文档中的词，还考虑了文档的整体信息。比如有一系列关于不同电影的评论文档，doc2vec 可以学习到每个文档的独特向量表示，同时也能体现出文档之间的相似性。

### 3.4 距离公式的本质

假设向量A=[a₁,a₂,a₃], B=[b₁,b₂,b₃]


| 方法       | 公式                              | 适用场景                 |
| ---------- | --------------------------------- | ------------------------ |
| 余弦相似度 | Σ(aᵢbᵢ)/√(Σaᵢ²)√(Σbᵢ²) | 文本检索（消除长度影响） |
| 欧氏距离   | √(Σ(aᵢ - bᵢ)²)               | 图像匹配/结构数据        |
| 曼哈顿距离 | Σ                                | aᵢ - bᵢ                |
| 点积相似度 | Σ(aᵢ*bᵢ)                       | 已归一化向量的快速计算   |
| 马氏距离   | √((A-B)ᵀ * Σ⁻¹ * (A-B))      | 考虑特征相关性的场景     |

### 3.5 混合检索策略
